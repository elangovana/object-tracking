{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MOT17 dataset\n",
    "\n",
    "Train MOt17 in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Set  up  accounts and role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tempfile\n",
    "\n",
    "sys.path.append('./src')\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "account_id =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "\n",
    "#role = sagemaker.get_execution_role()\n",
    "role=\"arn:aws:iam::{}:role/service-role/AmazonSageMaker-ExecutionRole-20190118T115449\".format(account_id)\n",
    "\n",
    "dataset_url = \"https://motchallenge.net/data/MOT17.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = \"./temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Setup image and instance type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_custom_image_name=\"object-tracking:gpu-1.1.0-201912122323\"\n",
    "instance_type = \"ml.p3.2xlarge\" \n",
    "docker_repo = \"{}.dkr.ecr.{}.amazonaws.com/{}\".format(account_id, region, pytorch_custom_image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_id =  \"fbdda42464389b1fe3d538b38b16e904fc46dcc6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Configure train/ test and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set prepare dataset to true if you want to download the 5 GB dataset from Mot 17 and update to s3. Once this is in s3 set the prepare_dataset false to avoid redoing this over and over again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sagemaker_session.default_bucket()\n",
    "raw_bucket=\"aegovansagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train=\"s3://{}/mot17v3/train/\".format(bucket)\n",
    "\n",
    "\n",
    "s3_val=\"s3://{}/mot17v3/val/\".format(bucket)\n",
    "\n",
    "s3_test=\"s3://{}/mot17v3/test/\".format(bucket)\n",
    "\n",
    "\n",
    "\n",
    "s3_output_path= \"s3://{}/market1501_output/\".format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import glob\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import argparse\n",
    "import datetime \n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def uploadfile(localpath, s3path):\n",
    "        \"\"\"\n",
    "Uploads a file to s3\n",
    "        :param localpath: The local path\n",
    "        :param s3path: The s3 path in format s3://mybucket/mydir/mysample.txt\n",
    "        \"\"\"\n",
    "        #print(\"Uploading {} to {}\".format(localpath, s3path))\n",
    "        \n",
    "        bucket, key = get_bucketname_key(s3path)\n",
    "\n",
    "        if key.endswith(\"/\"):\n",
    "            key = \"{}{}\".format(key, os.path.basename(localpath))\n",
    "        \n",
    "        s3 = boto3.client('s3')\n",
    "        \n",
    "        s3.upload_file(localpath, bucket, key)\n",
    "\n",
    "def get_bucketname_key(uripath):\n",
    "    assert uripath.startswith(\"s3://\")\n",
    "\n",
    "    path_without_scheme = uripath[5:]\n",
    "    bucket_end_index = path_without_scheme.find(\"/\")\n",
    "\n",
    "    bucket_name = path_without_scheme\n",
    "    key = \"/\"\n",
    "    if bucket_end_index > -1:\n",
    "        bucket_name = path_without_scheme[0:bucket_end_index]\n",
    "        key = path_without_scheme[bucket_end_index + 1:]\n",
    "\n",
    "    return bucket_name, key\n",
    "\n",
    "\n",
    "def download_file(s3path, local_dir):\n",
    "    bucket, key = get_bucketname_key(s3path)\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    local_file = os.path.join(local_dir, s3path.split(\"/\")[-1])\n",
    "    \n",
    "\n",
    "    s3.download_file(bucket, key, local_file)\n",
    "    \n",
    "def download_object(s3path):\n",
    "    bucket, key = get_bucketname_key(s3path)\n",
    "    \n",
    "    s3 = boto3.client('s3')    \n",
    "\n",
    "    s3_response_object = s3.get_object(Bucket=bucket, Key=key)\n",
    "    object_content = s3_response_object['Body'].read()\n",
    "    \n",
    "    return len(object_content)\n",
    "\n",
    "\n",
    "\n",
    "def list_files(s3path_prefix):\n",
    "    assert s3path_prefix.startswith(\"s3://\")\n",
    "    assert s3path_prefix.endswith(\"/\")\n",
    "    \n",
    "    bucket, key = get_bucketname_key(s3path_prefix)\n",
    "    \n",
    "   \n",
    "   \n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    bucket = s3.Bucket(name=bucket)\n",
    "\n",
    "    return ( (o.bucket_name, o.key) for o in bucket.objects.filter(Prefix=key))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upload_files(local_dir, s3_prefix, num_threads=20):    \n",
    "    s3_path = s3_prefix.rstrip(\"/\")\n",
    "    input_tuples = ( (f, \"{}/{}\".format( s3_path, f.lstrip(local_dir.lstrip(\"/\")))) for f in glob.glob(\"{}/**\".format(local_dir),  recursive=True) if os.path.isfile(f) )\n",
    "    list_input_tuples = list(input_tuples)\n",
    "    print(\"{} : Uploading {} files\".format(datetime.now(), len(list_input_tuples)))\n",
    "    with ThreadPool(num_threads) as pool:\n",
    "        pool.starmap(uploadfile, list_input_tuples)\n",
    "    print(\"{} : upload complete\".format(datetime.now(), len(list_input_tuples)))\n",
    "    \n",
    "\n",
    "\n",
    "def download_files(s3_prefix, local_dir, num_threads=20):    \n",
    "    input_tuples = ( (\"s3://{}/{}\".format(s3_bucket,s3_key),  local_dir) for s3_bucket, s3_key in list_files(s3_prefix))\n",
    "    \n",
    "    with ThreadPool(num_threads) as pool:\n",
    "        results = pool.starmap(download_file, input_tuples)\n",
    "        \n",
    "        \n",
    "\n",
    "def download_objects(s3_prefix, num_threads=20):    \n",
    "    s3_files = ( \"s3://{}/{}\".format(s3_bucket,s3_key) for s3_bucket, s3_key in list_files(s3_prefix))\n",
    "    \n",
    "    with ThreadPool(num_threads) as pool:\n",
    "        results = pool.map(download_object, s3_files)\n",
    "        \n",
    "    return sum(results)/1024\n",
    "        \n",
    "\n",
    "def get_directory_size(start_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            # skip if it is symbolic link\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "def split_train_val(train_dir_root, dest_train, dest_val, dest_test):\n",
    "    \"\"\"\n",
    "    Expects the root path containing MOT 17 directory structure as is. \n",
    "    \"\"\"\n",
    "    folders = [ (train_dir_root , d)  for d in os.listdir(train_dir_root) if os.path.isdir(os.path.join(train_dir_root, d))]\n",
    "    train_list, test_list = train_test_split(    folders , test_size=0.2, random_state=42)\n",
    "    train_list, val_list = train_test_split(    train_list , test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "    def partition(list_to_partition, dest_dir):\n",
    "        for root, d  in list_to_partition :\n",
    "            dest_path = os.path.join(dest_dir, d)\n",
    "            src_path =  os.path.join(train_dir_root, d)\n",
    "            print(\"moving {} to {} \".format(src_path, dest_path))\n",
    "            shutil.move(src_path, dest_path)\n",
    "            \n",
    "    partition(train_list, dest_train)\n",
    "    partition(val_list, dest_val)\n",
    "    partition(test_list, dest_test)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request \n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "\n",
    "def download_dataset(dest_file, url):\n",
    "    MB = 1024 * 1024 \n",
    "    read_size= MB * 5\n",
    "    log_freq=30\n",
    "    print(\"Downloading file {}....\".format(dest_file))\n",
    "    with  urllib.request.urlopen(url) as u:\n",
    "        with open(dest_file, \"wb\") as o:\n",
    "            total_read = 0\n",
    "            i = 0\n",
    "            while True:\n",
    "                buffer = u.read(read_size)\n",
    "                if not buffer:\n",
    "                    break\n",
    "                total_read += len(buffer)\n",
    "                o.write(buffer)\n",
    "                i += 1\n",
    "                if i % log_freq == 0 : print(\"{} so far...{} MB \".format(datetime.now(), total_read/MB))\n",
    "                \n",
    "\n",
    "def extract_zip(zip_file, dest_path):\n",
    "   \n",
    "    print(\"Is valid zip file: {}\".format(zipfile.is_zipfile(zip_file)))\n",
    "      \n",
    "    with zipfile.ZipFile(zip_file) as z:\n",
    "        print(\"Invalid members...{}\".format(z.testzip()))\n",
    "        \n",
    "    with zipfile.ZipFile(zip_file, 'r', allowZip64=True) as z: \n",
    "        print(\"Extracting files to {}....\".format(dest_path))\n",
    "        for member in z.infolist():\n",
    "            try:\n",
    "                z.extract(member, dest_path)\n",
    "            except zipfile.error as e:\n",
    "                Warning(e)\n",
    "        print(\"Completed..\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "%%bash -s  \"$prepare_dataset\" \"$dataset_url\" \"$tmp_dir\"\n",
    "\n",
    "prepare_dataset=$1\n",
    "dataset_url=$2\n",
    "download_dir=$3\n",
    "if [ \"$prepare_dataset\" == \"True\" ]\n",
    "then\n",
    "    echo  Downloading file $dataset_url to $download_dir\n",
    "    mkdir -p $download_dir\n",
    "    wget  -O ${download_dir}/mot17.zip  $dataset_url\n",
    "    echo  unziping file\n",
    "    unzip ${download_dir}/mot17.zip -d ${download_dir}/data\n",
    "fi\n",
    "echo  \"$prepare_dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not preparing dataset as prepare_dataset is set to False\n",
      "CPU times: user 63 µs, sys: 7 µs, total: 70 µs\n",
      "Wall time: 52.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "if prepare_dataset:\n",
    " \n",
    "    \n",
    "    raw_dataset_dir = os.path.join(tmp_dir, \"data\")\n",
    "     \n",
    "    \n",
    "#     download_dataset(raw_dataset_zip, dataset_url)\n",
    "#     extract_zip (raw_dataset_zip, raw_dataset_dir )\n",
    "    \n",
    "    dest_train = os.path.join(tmp_dir, \"train\")\n",
    "    dest_test = os.path.join(tmp_dir, \"test\")\n",
    "    dest_val = os.path.join(tmp_dir, \"val\")\n",
    "    \n",
    "    src_raw_train_dir = os.path.join(raw_dataset_dir, \"train\")\n",
    "    split_train_val(  src_raw_train_dir  , dest_train, dest_val, dest_test)\n",
    "    upload_files(dest_train, s3_train)\n",
    "    upload_files(dest_val, s3_val)\n",
    "    upload_files(dest_test, s3_test)\n",
    "\n",
    "else:\n",
    "    print(\"Not preparing dataset as prepare_dataset is set to False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"train\" : s3_train,\n",
    "     \"val\" :s3_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_exp1 = {\n",
    "    \"dataset\":\"Mot17DetectionFactory\",\n",
    "    \"batchsize\": \"32\",\n",
    "    \"epochs\" : \"1000\",\n",
    "    \"learning_rate\":.0001,\n",
    "    \"weight_decay\":5e-5,\n",
    "    \"momentum\":.9,\n",
    "    \"patience\": 20,\n",
    "    \"log-level\" : \"INFO\",\n",
    "    \"commit_id\":commit_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{\"Name\": \"TrainLoss\",\n",
    "                     \"Regex\": \"###score: train_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"ValidationLoss\",\n",
    "                     \"Regex\": \"###score: val_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"TrainScore\",\n",
    "                     \"Regex\": \"###score: train_score### (\\d*[.]?\\d*)\"}\n",
    "                   ,{\"Name\": \"ValidationScore\",\n",
    "                     \"Regex\": \"###score: val_score### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"trainVariance\",\n",
    "                     \"Regex\": \"###score: train_loss_std### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"ValVariance\",\n",
    "                     \"Regex\": \"###score: val_loss_std### (\\d*[.]?\\d*)\"}\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit fbdda42464389b1fe3d538b38b16e904fc46dcc6\n",
      "    Update notebook\n"
     ]
    }
   ],
   "source": [
    "!git log -1| head -1\n",
    "!git log -1| tail -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = hyperparameters_exp1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'Mot17DetectionFactory',\n",
       " 'batchsize': '32',\n",
       " 'epochs': '1000',\n",
       " 'learning_rate': 0.0001,\n",
       " 'weight_decay': 5e-05,\n",
       " 'momentum': 0.9,\n",
       " 'patience': 20,\n",
       " 'log-level': 'INFO',\n",
       " 'commit_id': 'fbdda42464389b1fe3d538b38b16e904fc46dcc6',\n",
       " 'sagemaker_submit_directory': 's3://sagemaker-us-east-2-324346001917/object-detection-2019-12-13-00-55-04-768/source/sourcedir.tar.gz',\n",
       " 'sagemaker_program': 'experiment_train.py',\n",
       " 'sagemaker_enable_cloudwatch_metrics': False,\n",
       " 'sagemaker_container_log_level': 20,\n",
       " 'sagemaker_job_name': 'object-detection-2019-12-13-00-55-04-768',\n",
       " 'sagemaker_region': 'us-east-2'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters_exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_config = {'repo': 'https://github.com/elangovana/object-tracking.git',\n",
    "              'branch': 'master',\n",
    "              'commit': hyperparameters['commit_id']\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "     entry_point='experiment_train.py',\n",
    "                    source_dir = 'src',\n",
    "                    dependencies =['src/datasets', 'src/evaluators', 'src/models'],\n",
    "                    role=role,\n",
    "                    framework_version =\"1.0.0\",\n",
    "                    py_version='py3',\n",
    "                    git_config= git_config,\n",
    "                    image_name= docker_repo,\n",
    "                    train_max_run = 60 * 60 * 24,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type=instance_type,\n",
    "                    hyperparameters =hyperparameters,\n",
    "                    output_path=s3_output_path,\n",
    "                    metric_definitions=metric_definitions,\n",
    "#                     train_use_spot_instances = True,\n",
    "#                     train_max_wait = 60 ,\n",
    "                    base_job_name =\"object-detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(inputs, wait=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
