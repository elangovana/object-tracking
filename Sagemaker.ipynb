{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MOT17 dataset\n",
    "\n",
    "Train MOt17 in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Set  up  accounts and role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "sys.path.append('./src')\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "account_id =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "\n",
    "#role = sagemaker.get_execution_role()\n",
    "role=\"arn:aws:iam::{}:role/service-role/AmazonSageMaker-ExecutionRole-20190118T115449\".format(account_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Setup image and instance type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_custom_image_name=\"object-tracking:gpu-1.1.0-201911080239\"\n",
    "instance_type = \"ml.p3.2xlarge\" \n",
    "docker_repo = \"{}.dkr.ecr.{}.amazonaws.com/{}\".format(account_id, region, pytorch_custom_image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Configure train/ test and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set prepare dataset to true if you want to download the 5 GB dataset from Mot 17 and update to s3. Once this is in s3 set the prepare_dataset false to avoid redoing this over and over again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sagemaker_session.default_bucket()\n",
    "raw_bucket=\"aegovansagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train=\"s3://{}/mot17/train/\".format(bucket)\n",
    "\n",
    "\n",
    "s3_val=\"s3://{}/mot17/val/\".format(bucket)\n",
    "\n",
    "s3_test=\"s3://{}/mot17/test/\".format(bucket)\n",
    "\n",
    "\n",
    "\n",
    "s3_output_path= \"s3://{}/market1501_output/\".format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import glob\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import argparse\n",
    "import datetime \n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def uploadfile(localpath, s3path):\n",
    "        \"\"\"\n",
    "Uploads a file to s3\n",
    "        :param localpath: The local path\n",
    "        :param s3path: The s3 path in format s3://mybucket/mydir/mysample.txt\n",
    "        \"\"\"\n",
    "\n",
    "        bucket, key = get_bucketname_key(s3path)\n",
    "\n",
    "        if key.endswith(\"/\"):\n",
    "            key = \"{}{}\".format(key, os.path.basename(localpath))\n",
    "        \n",
    "        s3 = boto3.client('s3')\n",
    "        \n",
    "        s3.upload_file(localpath, bucket, key)\n",
    "\n",
    "def get_bucketname_key(uripath):\n",
    "    assert uripath.startswith(\"s3://\")\n",
    "\n",
    "    path_without_scheme = uripath[5:]\n",
    "    bucket_end_index = path_without_scheme.find(\"/\")\n",
    "\n",
    "    bucket_name = path_without_scheme\n",
    "    key = \"/\"\n",
    "    if bucket_end_index > -1:\n",
    "        bucket_name = path_without_scheme[0:bucket_end_index]\n",
    "        key = path_without_scheme[bucket_end_index + 1:]\n",
    "\n",
    "    return bucket_name, key\n",
    "\n",
    "\n",
    "def download_file(s3path, local_dir):\n",
    "    bucket, key = get_bucketname_key(s3path)\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    local_file = os.path.join(local_dir, s3path.split(\"/\")[-1])\n",
    "    \n",
    "\n",
    "    s3.download_file(bucket, key, local_file)\n",
    "    \n",
    "def download_object(s3path):\n",
    "    bucket, key = get_bucketname_key(s3path)\n",
    "    \n",
    "    s3 = boto3.client('s3')    \n",
    "\n",
    "    s3_response_object = s3.get_object(Bucket=bucket, Key=key)\n",
    "    object_content = s3_response_object['Body'].read()\n",
    "    \n",
    "    return len(object_content)\n",
    "\n",
    "\n",
    "\n",
    "def list_files(s3path_prefix):\n",
    "    assert s3path_prefix.startswith(\"s3://\")\n",
    "    assert s3path_prefix.endswith(\"/\")\n",
    "    \n",
    "    bucket, key = get_bucketname_key(s3path_prefix)\n",
    "    \n",
    "   \n",
    "   \n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    bucket = s3.Bucket(name=bucket)\n",
    "\n",
    "    return ( (o.bucket_name, o.key) for o in bucket.objects.filter(Prefix=key))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upload_files(local_dir, s3_prefix, num_threads=20):    \n",
    "    s3_path = s3_prefix.rstrip(\"/\")\n",
    "    input_tuples = ( (f, \"{}/{}\".format( s3_path, f.lstrip(local_dir.lstrip(\"/\")))) for f in glob.glob(\"{}/**\".format(local_dir),  recursive=True) if os.path.isfile(f) )\n",
    "    print(\"{} : Uploading {} files\".format(datetime.now(), len(list(input_tuples))))\n",
    "    with ThreadPool(num_threads) as pool:\n",
    "        pool.starmap(uploadfile, input_tuples)\n",
    "    print(\"{} : upload complete\".format(datetime.now(), len(list(input_tuples))))\n",
    "    \n",
    "\n",
    "\n",
    "def download_files(s3_prefix, local_dir, num_threads=20):    \n",
    "    input_tuples = ( (\"s3://{}/{}\".format(s3_bucket,s3_key),  local_dir) for s3_bucket, s3_key in list_files(s3_prefix))\n",
    "    \n",
    "    with ThreadPool(num_threads) as pool:\n",
    "        results = pool.starmap(download_file, input_tuples)\n",
    "        \n",
    "        \n",
    "\n",
    "def download_objects(s3_prefix, num_threads=20):    \n",
    "    s3_files = ( \"s3://{}/{}\".format(s3_bucket,s3_key) for s3_bucket, s3_key in list_files(s3_prefix))\n",
    "    \n",
    "    with ThreadPool(num_threads) as pool:\n",
    "        results = pool.map(download_object, s3_files)\n",
    "        \n",
    "    return sum(results)/1024\n",
    "        \n",
    "\n",
    "def get_directory_size(start_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            # skip if it is symbolic link\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "def split_train_val(train_dir_root, dest_train, dest_val, dest_test):\n",
    "    \"\"\"\n",
    "    Expects the root path containing MOT 17 directory structure as is. \n",
    "    \"\"\"\n",
    "    folders = [ (train_dir_root , d)  for d in os.listdir(train_dir_root) if os.path.isdir(os.path.join(train_dir_root, d))]\n",
    "    train_list, test_list = train_test_split(    folders , test_size=0.2, random_state=42)\n",
    "    train_list, val_list = train_test_split(    train_list , test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "    def partition(list_to_partition, dest_dir):\n",
    "        for root, d  in list_to_partition :\n",
    "            dest_path = os.path.join(dest_dir, d)\n",
    "            src_path =  os.path.join(train_dir_root, d)\n",
    "            print(\"moving {} to {} \".format(src_path, dest_path))\n",
    "            shutil.move(src_path, dest_path)\n",
    "            \n",
    "    partition(train_list, dest_train)\n",
    "    partition(val_list, dest_val)\n",
    "    partition(test_list, dest_test)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not preparing dataset as prepare_dataset is set to False\n",
      "CPU times: user 160 µs, sys: 66 µs, total: 226 µs\n",
      "Wall time: 187 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import tempfile\n",
    "\n",
    "if prepare_dataset:\n",
    "    # TODO: Download from MOT 17 url\n",
    "    raw_dataset = \"~/Downloads/MOT17/train\"\n",
    "    tmp_dir = tempfile.mkdtemp(\"mot17data\")\n",
    "    \n",
    "    #tmp_dir= \"/var/folders/sm/0jbn3m7x7gg6sxqyq1h3lmgj6h25fm/T/tmpdl_m8q6wmot17data\"\n",
    "    dest_train = os.path.join(tmp_dir, \"train\")\n",
    "    dest_test = os.path.join(tmp_dir, \"test\")\n",
    "    dest_val = os.path.join(tmp_dir, \"val\")\n",
    "    \n",
    "    split_train_val(raw_dataset, dest_train, dest_val, dest_test)\n",
    "    upload_files(dest_train, s3_train)\n",
    "    upload_files(dest_val, s3_val)\n",
    "    upload_files(dest_test, s3_test)\n",
    "\n",
    "else:\n",
    "    print(\"Not preparing dataset as prepare_dataset is set to False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"train\" : s3_train,\n",
    "    \"val\" :s3_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"dataset\":\"Mot17DetectionFactory\",\n",
    "    \"batchsize\": \"32\",\n",
    "    \"epochs\" : \"1000\",\n",
    "    \"learning_rate\":.0001,\n",
    "    \"weight_decay\":5e-5,\n",
    "    \"momentum\":.9,\n",
    "    \"patience\": 20,\n",
    "    \"log-level\" : \"INFO\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{\"Name\": \"TrainLoss\",\n",
    "                     \"Regex\": \"###score: train_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"ValidationLoss\",\n",
    "                     \"Regex\": \"###score: val_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"TrainScore\",\n",
    "                     \"Regex\": \"###score: train_score### (\\d*[.]?\\d*)\"}\n",
    "                   ,{\"Name\": \"ValidationScore\",\n",
    "                     \"Regex\": \"###score: val_score### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"trainVariance\",\n",
    "                     \"Regex\": \"###score: train_loss_std### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"ValVariance\",\n",
    "                     \"Regex\": \"###score: val_loss_std### (\\d*[.]?\\d*)\"}\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit 0a8c60101a232e750f02ee83ec6829fa9fbce51c\n",
      "    Try  python conda\n"
     ]
    }
   ],
   "source": [
    "!git log -1| head -1\n",
    "!git log -1| tail -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_config = {'repo': 'https://github.com/elangovana/object-tracking.git',\n",
    "              'branch': 'master',\n",
    "             # 'commit': 'd14df4a3847e74c5672dae42304c0caa5a5c1ae2'\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "     entry_point='experiment_train.py',\n",
    "                    source_dir = 'src',\n",
    "                    dependencies =['src/datasets', 'src/evaluators', 'src/models'],\n",
    "                    role=role,\n",
    "                    framework_version =\"1.0.0\",\n",
    "                    py_version='py3',\n",
    "                   # git_config= git_config,\n",
    "                    image_name= docker_repo,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type=instance_type,\n",
    "                    hyperparameters =hyperparameters,\n",
    "                    output_path=s3_output_path,\n",
    "                    metric_definitions=metric_definitions,\n",
    "                    #train_use_spot_instances = True\n",
    "                    base_job_name =\"object-detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-19 05:29:42 Starting - Starting the training job...\n",
      "2019-11-19 05:29:44 Starting - Launching requested ML instances...\n",
      "2019-11-19 05:30:37 Starting - Preparing the instances for training.."
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
